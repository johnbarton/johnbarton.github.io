<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><title>ACE/</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="alternate" type="application/rss+xml" title="John Barton" href="http://johnbarton.github.io/feed.xml"><link href="/css/style.css" rel="stylesheet" media="screen"> <script>!function(e){var t,a={kitId:"xeu8jut",scriptTimeout:3e3},c=e.documentElement,i=setTimeout(function(){c.className=c.className.replace(/\bwf-loading\b/g,"")+" wf-inactive"},a.scriptTimeout),n=e.createElement("script"),o=!1,s=e.getElementsByTagName("script")[0];c.className+=" wf-loading",n.src="//use.typekit.net/"+a.kitId+".js",n.async=!0,n.onload=n.onreadystatechange=function(){if(t=this.readyState,!(o||t&&"complete"!=t&&"loaded"!=t)){o=!0,clearTimeout(i);try{Typekit.load(a)}catch(e){}}},s.parentNode.insertBefore(n,s)}(document)</script> <script>!function(e,t,a,n,c,s,o){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,s=t.createElement(a),o=t.getElementsByTagName(a)[0],s.async=1,s.src=n,o.parentNode.insertBefore(s,o)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-89355185-1","auto"),ga("send","pageview")</script> <script src="/js/jquery.min.js"></script> <script src="/js/transition.js"></script><script src="/js/collapse.js"></script> <script src="/js/katex.min.js"></script><link href="/css/katex.min.css" rel="stylesheet" type="text/css"> <script src="/js/auto-render.min.js"></script></head><body><div id="header"> <nav class="navbar navbar-default navbar-static-top" role="navigation"><div class="container"><div class="navbar-header"> <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-element" aria-expanded="false"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/">John Barton</a></div><div class="collapse navbar-collapse" id="navbar-collapse-element"><ul class="nav navbar-nav navbar-right"><li> <a href="/blog/">notes</a></li><li> <a href="/papers/">papers</a></li><li> <a href="/projects/">code</a></li></ul></div></div> </nav></div><div class="container"><div class="row"><div class="col-md-12"><div class="bigtitle titlebox"><ol class="breadcrumb"><li>ACE</li></ol></div><p><div class="head"> Fast and flexible code for solving the inverse Ising/Potts inference problem</div></div></div><div class="bigspacer"></div><div class="row"><div class="col-md-3"><div class="bigspacer"></div><div class="smallhead"> Source code</div><div class="pad-left note"><div class="smallspacer"></div> <i class="fa fa-cog fa-fw"></i> <a class="off" href="https://github.com/johnbarton/ACE">github.com/johnbarton/ACE</a></div><div class="bigspacer"></div><div class="smallhead"> Contributors</div><div class="pad-left note"><div class="smallspacer"></div><div> <a class="off" href="https://github.com/johnbarton"> <img class="pull-left avatar" src="https://avatars3.githubusercontent.com/u/2357804?v=3&s=50"><div class="handlebox" style="padding-left:5px"> johnbarton</div> </a></div></div><div class="bigspacer"></div><div class="smallhead"> Latest commits</div><div class="pad-left smallnote"><ul class="list-unstyled"><div class="smallspacer"></div><li> <i class="fa fa-check-square-o fa-fw"></i> <a class="off" href="https://github.com/johnbarton/ACE/commit/ac6f9ae484a1557b9e69315951aa3a920d961a6c"> 29 May 2017 - <span class="text-gray">renamed cluster overlap to cluster cover</span> </a></li><div class="smallspacer"></div><li> <i class="fa fa-check-square-o fa-fw"></i> <a class="off" href="https://github.com/johnbarton/ACE/commit/c379215079b8f192ee2afd8e7f8e9c14d72318f7"> 29 May 2017 - <span class="text-gray">added optional output of significant cluster superset</span> </a></li><div class="smallspacer"></div><li> <i class="fa fa-check-square-o fa-fw"></i> <a class="off" href="https://github.com/johnbarton/ACE/commit/4f0b754b023392787d9a7a4e6d83eaef20e0207a"> 14 Apr 2017 - <span class="text-gray">added script for conversion from consensus to zero sum gauge</span> </a></li><div class="smallspacer"></div><li> <i class="fa fa-check-square-o fa-fw"></i> <a class="off" href="https://github.com/johnbarton/ACE/commit/94cd9f1203be5a55071c1bc2222bbccf70193763"> 05 Mar 2017 - <span class="text-gray">added __init__.py to /scripts for easy import</span> </a></li><div class="smallspacer"></div><li> <i class="fa fa-check-square-o fa-fw"></i> <a class="off" href="https://github.com/johnbarton/ACE/commit/d01204b0998e4b84e7c1b8a5fc3206dbcb890992"> 24 Jan 2017 - <span class="text-gray">normalized sign convention for regularization</span> </a></li></ul></div><div class="spacer"></div></div><div class="col-md-8"><div class="post" id="post"><p> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h4>ACE <a href="https://travis-ci.org/johnbarton/ACE"><img src="https://travis-ci.org/johnbarton/ACE.svg?branch=master" alt="Build Status"></a></h4><h1>Table of contents</h1><p>[TOC]</p><h1>Introduction</h1><p>ACE is a software package designed to quickly and accurately infer <a href="https://en.wikipedia.org/wiki/Ising_model">Ising</a> or <a href="https://en.wikipedia.org/wiki/Potts_model">Potts</a> models based on correlation data from a variety of biological and artificial systems. This software makes use of the <b>A</b>daptive <b>C</b>luster <b>E</b>xpansion (ACE) algorithm.</p><p>Given a set of correlation data or sequence input in <a href="http://en.wikipedia.org/wiki/FASTA_format">FASTA</a> format, ACE will produce a Ising or Potts model that reproduces the input correlations to within the expected error due to finite sampling.</p><p><b>NOTE:</b> Mathematical expressions through MathJax are currently not supported on GitHub. To see these expressions rendered properly, please see <code>README.pdf</code> or the alternative <a href="http://johnbarton.github.io/projects/ACE/">project page</a>.</p><h1>Installation</h1><p>Download and unzip the package, then run the following commands in the terminal from the new directory:</p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./configure
<span class="gp">$ </span>make
</code></pre></div><p>If you'd like to be able to run the program from any directory, you can then enter:</p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>make install
</code></pre></div><h1>Required Input</h1><p>Running the algorithm requires a set of correlations as input, to be computed from your data.</p><p>As an example, let's consider a system of $N$ variables described by the configuration $\underline{x}={x_1, x_2,\ldots,x_N}$, with each variable $x_i$ taking one of $q_i$ possible values, $x_i\in{1,2,\ldots,q_i}$. From a set of $B$ observations of the system, we can compute the frequency of each variable as well as the pairwise correlations,</p><p>$$ \begin{aligned} \begin{align} p_i(a) &amp;= \frac{1}{B}\sum_{k=1}^{B}\delta(x_i,a)\,,\ p_{ij}(a,b) &amp;= \frac{1}{B}\sum_{k=1}^{B}\delta(x_i,a)\delta(x_j,b)\,. \end{align} \end{aligned} $$</p><p>Here $\delta$ represents the <a href="http://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta function</a>. These correlations should be saved in a file ending with the extension <code>.p</code>, in the following format:</p><blockquote><p>$p_1(1)$ $p_1(2)$ ... $p_1(q_1-1)$ $p_2(1)$ $p_2(2)$ ... $p_2(q_2-1)$ ... $p_N(1)$ $p_N(2)$ ... $p_N(q_N-1)$ $p_{1,2}(1,1)$ $p_{1,2}(1,2)$ ... $p_{1,2}(1,q_2-1)$ $p_{1,2}(2,1)$ $p_{1,2}(2,2)$ ... $p_{1,2}(q_1-1,q_2-1)$ $p_{1,3}(1,1)$ ...</p></blockquote><p>In other words, the first $N$ lines of the file record the frequency that each state is observed at each site, and the next $N(N-1)/2$ lines record the pairwise correlations. Note that, because $\sum_{a=1}^{q_i} p_i(a)=1$, the frequency (and corresponding pair correlations) for one state at each site need not be specified explicitly.</p><p>These values should be given in floating point or scientific format, with whitespace (e.g. <code>'\t'</code>) between successive values and a newline character (<code>'\n'</code>) at the end of each line. In order for the correlations to be read in properly, there should be <strong>no</strong> whitespace between the final correlation value and the newline character on each line.</p><p>For examples, see the <code>examples/</code> directory. Instructions on how to automatically generate correlations from a sequence alignment in FASTA format (and others) can be found in the Matlab file in the <code>scripts/</code> directory. The correlations can also be prepared through a set of Python scripts, <code>scripts/ACEtools.py</code>, which also includes useful auxiliary functions.</p><h1>Running the program</h1><p>Here we show a simple example of how to run the program and interpret the output, using a set of sample data for the HIV protein p7. Full explanations for the possible options are given <a href="#command-line-options">here</a>.</p><h3>Running ACE</h3><p>We begin running the ACE algorithm on the example p7 dataset with the command:</p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./bin/ace -d examples -i p7 -o p7-out -g2 0.0002 -b 4130
</code></pre></div><p>This creates two new files, <code>examples/p7-out.sce</code> and <code>examples/p7-out.j</code>, which record general output on the inference procedure and the current inferred Potts parameters, respectively.</p><p>Output from the first file, <code>examples/p7-out.sce</code>, should appear something like the following:</p><blockquote><p>8.463532e-04 1.917385e+01 9.964734e+00 1.309919e+02 1.186601e+01 4 3321 364 8.060507e-04 1.911941e+01 1.019924e+01 1.334989e+02 1.186352e+01 4 3343 373 7.676673e-04 2.298425e+01 1.194769e+01 1.659755e+02 1.186115e+01 4 3353 380 7.311117e-04 2.550650e+01 1.277885e+01 1.768295e+02 1.185510e+01 4 3399 390 6.962969e-04 1.842427e+01 9.891297e+00 1.219612e+02 1.185371e+01 4 3442 406 ...</p></blockquote><p>These columns represent, respectively: the current value of the threshold $\theta$, error on the one-point correlations $\epsilon_{p1}$, error on the pairwise correlations $\epsilon_{p2}$, normalized maximum error $\epsilon_{\rm max}$, current estimate of the entropy $S$, maximum cluster size, total number of clusters in the expansion, and the number of selected clusters (i.e. those for which $| \Delta S |&gt;\theta$).</p><p>The inferred Potts parameters in the second file, <code>examples/p7-out.j</code>, are output in the same format as the input correlations, as shown <a href="#required-input">above</a>. In this case, the first $N$ lines record the Potts fields $h_i(a)$, and the following $N(N-1)/2$ lines record the couplings $J_{ij}(a,b)$.</p><p>The final line of <code>examples/p7-out.sce</code> should then appear something like:</p><blockquote><p>1.338016e-05 3.641289e-01 1.674574e-01 9.498217e-01 1.169706e+01 6 20293 4773</p></blockquote><p>The error for the Potts parameters is low (the error terms $\epsilon_{p1}, \epsilon_{p2}, \epsilon_{\rm max}<1$), but we can follow this initial inference step by running the <b>M</b>onte <b>C</b>arlo (MC) learning algorithm to ensure convergence. This is particularly useful when convergence is difficult to obtain in the cluster algorithm alone. Typically we find that MC learning is more likely to be successful when the entropy has nearly converged (see column 6 in <code>examples/p7-out.sce</code>).</p><h3>Running the MC learning algorithm QLS</h3><p>We now run the MC algorithm on the output we previously obtained from ACE, using the command:</p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./bin/qls -d examples -c p7 -i p7-out -o p7-out-learn -g2 0.0002 -b 4130
</code></pre></div><p>This creates two additional output files, <code>examples/p7-out-learn.fit</code> and <code>examples/p7-out-learn.j</code>, which record progress on the MC learning procedure and the current refined Potts parameters, respectively.</p><p>Output from the first file, <code>examples/p7-out-learn.fit</code>, should appear something like the following:</p><blockquote><p>1 1.877327e-01 1.266454e-01 1.303780e+00 1.900000e+00 2 1.976668e-01 1.284943e-01 1.329022e+00 3.610000e+00 3 1.956319e-01 1.276321e-01 1.340715e+00 6.859000e+00 4 1.959972e-01 1.282958e-01 1.279392e+00 1.303210e+01 5 2.073654e-01 1.295238e-01 1.377221e+00 2.476099e+01 ...</p></blockquote><p>These columns represent, respectively: the current iteration, error on the one-point correlations $\epsilon_{p1}$, error on the pairwise correlations $\epsilon_{p2}$, normalized maximum error $\epsilon_{\rm max}$, and the maximum size of the weight parameter used in the MC learning update step. Note that the error is slightly different in this case than at the end of the cluster algorithm. This is because, by default, the MC learning algorithm computes the correlations using a larger number of samples.</p><p>After about 15 iterations the MC learning algorithm should converge and the program will terminate. The Potts parameters recorded in the second file, <code>examples/p7-out-learn.j</code>, now specify a model that accurately recovers the input correlations to within fluctuations expected due to finite sampling.</p><h3>Verifying the output with QGT</h3><p>If the input correlations are generated using the matlab script included in this package (for instructions, see the script itself, which lies in the <code>scripts/</code> directory), auxiliary measurements such as higher order correlations can be checked comprehensively. By default, this routine compares the one- and two-point correlations for the model and data, as well as the probability $P(k)$ of observing $k$ differences between sampled configurations and the "consensus" (determined from input read in to the program). It is also possible to compute the three-point correlations, the energy distribution, and a set of sample configurations.</p><p>This routine can be run using, for example:</p><div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>./bin/qgt -d examples -c p7 -m p7 -w p7 -i p7-out -o p7-out-fit -g2 0.0002 -b 4130
</code></pre></div><p>This program also gives the RMS error and Pearson correlation between the model and data correlations.</p><p>Note that the output correlations from this program include not just the input ones, but also the ones corresponding to the "gauged" states, which are implicit in the input data. We also note that by default the output of small three-point correlations is trimmed in order to prevent generating extremely large files. For more information, see the options below.</p><h1>Troubleshooting</h1><p>On difficult data sets (for example, systems of very large size, those with many states, and/or high variability), ACE may be more slow to converge. Below are a few common potential problems and suggestions for how to fix them.</p><p><strong>The one- and two-point error terms have converged ($\epsilon\leq 1$), but the maximum error remains $&gt; 1$.</strong> Obtaining a normalized maximum error $&lt; 1$ is the most stringent statistical check of the inferred model performed by the ACE and QLS routines. It is possible to obtain a very good generative model of the data even in cases where this error is larger than 1 -- use QGT to verify an acceptable fit.</p><p><strong>The entropy is oscillating.</strong> Large oscillations of the entropy can be observed when whole collections of variables strongly interact. One common source is strongly-correlated gaps at the beginning and end of sequences from protein families. In such cases, filling in gaps while computing the correlations, using stronger compression, or increasing the regularization strength can help to reduce oscillations and improve convergence.</p><p><strong>ACE slows down as the size of clusters increases.</strong> This can occur if the number of states is very large. Stronger compression can allow the cluster expansion algorithm to proceed further. In addition, QLS can often converge rapidly after the network of strong interactions has been inferred by ACE, even if the errors are high and ACE has not yet advanced to a low value of the threshold (see the lattice protein model <a href="http://dx.doi.org/10.1093/bioinformatics/btw328">here</a> for an example).</p><p><strong>Additional questions?</strong> Please contact us for more information or advice on dealing with difficult data sets.</p><h1>Command line options</h1><h3>Options for all programs</h3><ul><li><code>-d</code> gives the path to the directory where data files are located, and where output will be written (default: "." (current directory))</li><li><code>-i</code> specifies the name of the input file (excluding the extension, default: "input")</li><li><code>-o</code> specifies the name of the output file (excluding the extension, default: "output")</li><li><code>-v</code> enables verbose output</li><li><code>-b</code> tells the program how many samples were used to generate the input correlations, so that the expected error in the correlations due to finite sampling can be estimated (default: 1000)</li><li><code>-mcb</code> gives the number of Monte Carlo steps used to estimate the inference error (default: 40000 (ace), 800000 (qls, qgt))</li><li><code>-mcr</code> gives the number of independent Monte Carlo trajectories to use when estimating the inference error (default: 1)</li><li><code>-g2</code> sets the $L_2$-norm regularization strength (note that a natural value for this parameter is $1/B$, where $B$ is the number of samples used to generate the input correlations -- for contact prediction, it may be best to use strong regularization $\approx 1$, regardless of the number of samples, default: 0)</li><li><code>-ag</code> automatically sets the $L_2$-norm regularization strength equal to $1/B$, using the number of samples $B$ passed with the <code>-b</code> option</li><li><code>-gi</code> enable the alternate gauge-invariant form of the $L_2$ regularization for couplings (see <a href="http://dx.doi.org/10.1093/bioinformatics/btw328">here</a> for details)</li></ul><h3>Additional ACE options</h3><ul><li><code>-kmin</code> sets the minimum cluster size required before the program will terminate (default: 0)</li><li><code>-kmax</code> sets the maximum cluster size; the program terminates automatically after a cluster of this size is created (default: none)</li><li><code>-t</code> specifies a single value of the threshold $\theta$ at which the algorithm will run, then exit</li><li><code>-tmax</code> specifies the maximum (starting) value of the threshold (default: 1)</li><li><code>-tmin</code> specifies the minimum allowed value of the threshold; the program terminates automatically after $\theta$ falls below this minimum value (default: 1e-10)</li><li><code>-ts</code> specifies the logarithmic step size to for between successive values $\theta$, through $\theta_{i+1} = \theta_i / \theta_{\rm step}$ (default: 1.05)</li><li><code>-r</code> enables the expansion of the entropy $S$ around a mean-field reference entropy $S_{0}$, which may be helpful in particular for inferring models described by dense networks of weak interactions (note: works only if all variables are binary)</li><li><code>-g0</code> sets the $L_0$-norm regularization strength, and turns on $L_0$-norm regularization, enforcing sparsity for Potts couplings (default: 1e-4)</li><li><code>-l0</code> turns on $L_0$-norm regularization, but <strong>without</strong> setting the regularization strength</li><li><code>-ss</code> specifies an input "secondary structure" file used to specify the initial set of clusters to consider in the expansion</li><li><code>-lax</code> enables a laxer cluster construction rule, increasing the number of clusters included in the cluster expansion routine</li></ul><h3>Additional QLS options</h3><ul><li><code>-c</code> specifies the set of (true) correlations to compare with for the MC learning routine (default: "input")</li><li><code>-e</code> sets the maximum tolerable error threshold; the program will run until all of the error terms $\epsilon_{p1}, \epsilon_{p2}, \epsilon_{\rm max}&lt;e$ (default: 1)</li></ul><h3>Additional QGT options</h3><ul><li><code>-c</code> specifies the file giving the consensus sequence (default: "input")</li><li><code>-m</code> specifies the file containing the compressed representation of the data (i.e. the compressed MSA, default: "input")</li><li><code>-w</code> specifies the file with weights for each configuration in the data (default: "input")</li><li><code>-pthresh</code> sets the threshold for three-point correlations that will be printed (default: $10\times \langle p\rangle$, where $\langle p \rangle$ is the average one-point correlation)</li><li><code>-p3</code> enables computation and comparison of three-point correlations; note that by default not all correlations are printed (see <code>-pthresh</code> option above)</li><li><code>-p3full</code> enables computation and comparison of three-point correlations and sets <code>pthresh</code> to zero</li><li><code>-nmax</code> set the maximum number of three-point correlations to print, another way to control file size (default: none)</li><li><code>-msaout</code> enables output of sample configurations from the model and their energies</li></ul><h1>References</h1><ol><li><p><a href="http://dx.doi.org/10.1093/bioinformatics/btw328">Barton, J. P., De Leonardis, E., Coucke, A. and Cocco, S. (2016). ACE: adaptive cluster expansion for maximum entropy graphical model inference. <i>Bioinformatics</i>, doi:http://dx.doi.org/10.1093/bioinformatics/btw328</a>.</p></li><li><p><a href="http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.106.090601">Cocco, S. and Monasson, R. (2011). Adaptive Cluster Expansion for Inferring Boltzmann Machines with Noisy Data. <i>Physical Review Letters</i>, <b>106</b>, 090601</a>.</p></li><li><p><a href="http://link.springer.com/article/10.1007/s10955-012-0463-4#page-1">Cocco, S. and Monasson, R. (2012). Adaptive Cluster Expansion for the Inverse Ising Problem: Convergence, Algorithm and Tests.<i> Journal of Statistical Physics</i>, <b>147</b>(2), 252–314</a>.</p></li><li><p><a href="http://iopscience.iop.org/1742-5468/2013/03/P03002">Barton, J. and Cocco, S. (2013). Ising models for neural activity inferred via selective cluster expansion: structural and coding properties. <i>Journal of Statistical Mechanics: Theory and Experiment</i>, <b>2013</b>(03), P03002</a>.</p></li></ol><blockquote><p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p></blockquote></div></div><div class="col-md-1"></div></div></div><div id="footer"><span style="display:none">foo</span></div> <script>renderMathInElement(document.getElementById("post"),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\[",right:"\]",display:!0},{left:"$",right:"$",display:!1}]})</script></body></html>